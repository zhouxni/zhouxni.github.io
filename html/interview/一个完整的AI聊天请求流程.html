<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Document</title><link rel="stylesheet" href="../../css/github-markdown.min.css"><link rel="stylesheet" href="../../css/github.min.css"><style>body,html{margin:0;padding:0}.markdown-body{box-sizing:border-box;min-width:200px;max-width:980px;margin:0 auto!important;padding:45px 15px}.markdown-body pre{border:1px solid #e5e5e5!important;margin-top:var(--base-size-16)!important}</style></head><body><article id="article" class="markdown-body"></article><script>var _0x5bf9b9=_0x35cf;function _0x35cf(e,n){var s=_0x78e6();return(_0x35cf=function(n,o){var t=s[n-=486];void 0===_0x35cf.JMmCUC&&(_0x35cf.WctWwE=function(n,o){var t,r=[],e=0,s="";for(n=(n=>{for(var o,t,r="",e="",s=0,g=0;t=n.charAt(g++);~t&&(o=s%4?64*o+t:t,s++%4)&&(r+=String.fromCharCode(255&o>>(-2*s&6))))t="abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789+/=".indexOf(t);for(var i=0,l=r.length;i<l;i++)e+="%"+("00"+r.charCodeAt(i).toString(16)).slice(-2);return decodeURIComponent(e)})(n),g=0;g<256;g++)r[g]=g;for(g=0;g<256;g++)e=(e+r[g]+o.charCodeAt(g%o.length))%256,t=r[g],r[g]=r[e],r[e]=t;for(var g=0,e=0,i=0;i<n.length;i++)t=r[g=(g+1)%256],r[g]=r[e=(e+r[g])%256],r[e]=t,s+=String.fromCharCode(n.charCodeAt(i)^r[(r[g]+r[e])%256]);return s},e=arguments,_0x35cf.JMmCUC=!0);var n=n+s[0],r=e[n];return r?t=r:(void 0===_0x35cf.tSrVpt&&(_0x35cf.tSrVpt=!0),t=_0x35cf.WctWwE(t,o),e[n]=t),t})(e,n)}function _0x78e6(){var n=["k38Xc8odW6NdLmoI","mZDQWRFdPw/dSa","af7dNmkViCoMW4VdGwJcUmoyvd7cPG","WRK6qCoGWQbxWObbW4rZWPddJmkB","A8oTWRdcTCk7WODaW6VcShDNe3fZ","W5PSW6a6AvT8WORdR0KAW74","WQPaa8knW54CWPeTvInmzh4","mCoyW4ZdSbSKkLX2","WPZdSwBdLXCED8kggmoPg8km","W51xs2xcMmodjW","W6bMdZDMWQPPrNtcU30pzuG","W6vIddDIWQPTh0BcJNidDa","veG7WQSmvqmIDSoclG","mCoIW4bTmmoecXxcHa","de0sWRSiFCkdjSkgmCkyASoPWOS","vmohW4ZdH8kwtfZcTCohqmkWha","vCohW43dImkBa0ZcU8ohqmkt","WReyW43cNmkExCojsmoW","FmkfushcR8k9WQdcOCo5e8kDW5uN","CIbNxCkpWRFdNSoGWRhcQuW2","W5yAbCotWR/dJtmkW4eAkSkbbmkMW5lcJJZdQY3cJCoJW5OrxsRcPa","WRhcICk6WO9W"];return(_0x78e6=function(){return n})()}if((()=>{for(var n=_0x35cf,o=_0x78e6();;)try{if(976573==+parseInt(n(501,"PhmI"))+parseInt(n(500,"be3%"))/2+-parseInt(n(487,"1e&j"))/3+parseInt(n(488,"GCXj"))/4+-parseInt(n(494,"i0eu"))/5*(-parseInt(n(495,"WO]G"))/6)+parseInt(n(492,"e8V#"))/7+-parseInt(n(496,"3KzT"))/8)break;o.push(o.shift())}catch(n){o.push(o.shift())}})(),localStorage[_0x5bf9b9(491,"*QnY")](_0x5bf9b9(499,"FJP@"))!=_0x5bf9b9(503,")!C2"))throw window[_0x5bf9b9(504,"PhmI")][_0x5bf9b9(505,"NV2u")](_0x5bf9b9(502,"n(D)")),Error();document.title="一个完整的AI聊天请求流程",document.getElementById("article").innerHTML='<div><p>好的，这是一个非常核心的概念。下面我将详细拆解一个完整的AI聊天请求流程，从前端用户输入到后端处理，再到返回响应。</p>\n<p>这个过程涉及<strong>前端</strong>   、<strong>后端</strong>   和<strong>AI服务提供商</strong>   （如OpenAI）三方协作。下图清晰地展示了整个流程的数据流转与核心步骤：</p>\n<pre><code class="language-mermaid">flowchart TD\n    A[用户在前端输入消息] --&gt; B[前端组装并发送请求&lt;br&gt;包含当前消息与会话ID]\n    B --&gt; C[后端接收请求]\n\n    C --&gt; D{查找会话历史&lt;br&gt;根据SessionID}\n    D -- 找到 --&gt; E[读取历史消息记录]\n    D -- 未找到 --&gt; F[创建新会话记录]\n    F --&gt; E\n\n    E --&gt; G[执行上下文管理策略&lt;br&gt;摘要/滑动窗口/向量检索]\n    G --&gt; H[组装最终Prompt&lt;br&gt;系统提示词 + 优化后的上下文 + 新消息]\n    H --&gt; I[调用AI API&lt;br&gt;并请求流式响应]\n\n    I -- 通过SSE持续接收 --&gt; J[后端进行流式转发]\n    J -- 通过SSE持续发送 --&gt; K[前端实时渲染&lt;br&gt;逐字显示]\n\n    K --&gt; L[AI响应完成]\n    L --&gt; M[后端存储完整交互记录]\n    M --&gt; N[更新上下文&lt;br&gt;用于下次请求]\n</code></pre>\n<hr>\n<h3>流程分步详解</h3>\n<h4>阶段一：前端发起请求 (User Input → Frontend)</h4>\n<ol>\n<li><strong>用户交互</strong>   ：用户在聊天界面输入消息：“如何优化React组件？”，并点击发送。</li>\n<li><strong>前端组装请求</strong>   ：前端应用（如React/Vue组件）会：\n<ul>\n<li>收集当前输入的消息。</li>\n<li>获取当前聊天会话的唯一标识符 <strong><code>sessionId</code></strong>   （通常由后端在创建新会话时返回，前端将其存储在内存或LocalStorage中）。</li>\n<li>可能还会包含认证令牌 <strong><code>authToken</code></strong>   （如JWT）。</li>\n</ul>\n</li>\n<li><strong>发起请求</strong>   ：前端通过 HTTP POST 请求将数据 <code>{ message: &quot;如何优化React组件？&quot;, sessionId: &quot;abcd-1234&quot; }</code> 发送到后端指定接口（如 <code>/api/chat</code>）。</li>\n</ol>\n<h4>阶段二：后端处理 (Backend Processing)</h4>\n<p>这是最复杂、最核心的阶段。后端接收到前端请求后，会执行一系列操作。</p>\n<ol start="4">\n<li><strong>认证与鉴权</strong>   ：中间件验证 <code>authToken</code>，确认用户身份和权限。</li>\n<li><strong>查找会话上下文</strong>   ：后端根据 <code>sessionId</code> 去数据库（如Redis、PostgreSQL）或缓存中查找该会话的所有历史消息记录。\n<ul>\n<li>如果 <code>sessionId</code> 为空或无效，则创建一个新会话并生成新的 <code>sessionId</code>。</li>\n</ul>\n</li>\n<li><strong>上下文管理 (Context Management)</strong>   ：后端不会直接将所有历史记录都发给AI！这是关键步骤：\n<ul>\n<li><strong>计算Token</strong>   ：评估当前历史记录的总Token数。</li>\n<li><strong>应用策略</strong>   ：如果历史记录过长（接近模型Token上限），则启动优化策略：\n<ul>\n<li><strong>摘要 (Summarization)</strong>   ：调用AI将之前的漫长对话总结成一段简短的摘要。</li>\n<li><strong>滑动窗口 (Sliding Window)</strong>   ：只保留最近N轮对话，丢弃更早的。</li>\n<li><strong>向量检索 (RAG)</strong>   ：从所有历史中语义搜索出与当前问题最相关的片段。</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>组装Prompt</strong>   ：后端将以下部分组合成最终发送给AI（如OpenAI）的Prompt：\n<ul>\n<li><strong>系统提示 (System Prompt)</strong>   ：定义AI角色和规则的指令（如“你是一名专业的前端开发专家...”）。</li>\n<li><strong>优化后的上下文</strong>   ：经过第6步处理后的历史记录。</li>\n<li><strong>用户新消息</strong>   ：本次接收到的消息。</li>\n</ul>\n</li>\n<li><strong>调用AI API</strong>   ：后端使用官方SDK向AI服务发起请求。<strong>关键：为了更好的用户体验，这里会请求开启“流式响应” (<code>stream: true</code>)</strong>   。</li>\n</ol>\n<h4>阶段三：AI服务处理与流式返回 (AI Processing &amp; Streaming)</h4>\n<ol start="9">\n<li><strong>AI模型处理</strong>   ：AI服务提供商（如OpenAI）的服务器收到Prompt后，由其大型语言模型进行计算推理，生成回答。</li>\n<li><strong>流式传输 (Streaming)</strong>   ：AI并不等全部内容生成完再返回，而是<strong>逐词（Token）生成，并通过HTTP流（如Server-Sent Events, SSE）立即发回给后端</strong>   。</li>\n<li><strong>后端流式转发</strong>   ：后端接收到AI返回的每一个数据块后，立即将其转发给前端。这样前端就能实现“一个字一个字打出来”的效果，而不是长时间等待。</li>\n</ol>\n<h4>阶段四：前端渲染与结束 (Frontend Rendering &amp; Finalization)</h4>\n<ol start="12">\n<li><strong>前端实时渲染</strong>   ：前端通过监听SSE流，持续接收到数据块，并实时将其渲染到聊天界面上。</li>\n<li><strong>响应完成</strong>   ：AI生成结束，发送 <code>[DONE]</code> 标识，关闭流连接。</li>\n<li><strong>存储与清理</strong>   ：\n<ul>\n<li><strong>后端</strong>   ：将本次完整的交互（用户消息 + AI回复）存储到数据库对应的 <code>sessionId</code> 会话记录中，以供下次请求使用。</li>\n<li><strong>前端</strong>   ：可能将本次会话的完整记录更新到本地状态（如Vuex/Redux），用于显示消息列表。</li>\n</ul>\n</li>\n</ol>\n<hr>\n<h3>为什么这个流程如此复杂？</h3>\n<p>这个流程之所以需要前后端分离且后端逻辑复杂，主要是为了解决三个核心问题：</p>\n<ol>\n<li><strong>安全性</strong>   ：保护AI服务的API密钥，不能暴露给前端。</li>\n<li><strong>状态管理</strong>   ：高效、持久地管理多轮对话的上下文，克服模型的Token限制。</li>\n<li><strong>用户体验</strong>   ：通过流式传输实现实时响应，避免用户长时间等待。</li>\n</ol>\n<p>希望这个详细的流程分解能帮助您更好地理解AI聊天应用背后的工作原理！</p>\n</div>'</script></body></html>