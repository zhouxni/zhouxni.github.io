<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Document</title><link rel="stylesheet" href="../../css/github-markdown.min.css"><link rel="stylesheet" href="../../css/github.min.css"><style>body,html{margin:0;padding:0}.markdown-body{box-sizing:border-box;min-width:200px;max-width:980px;margin:0 auto!important;padding:45px 15px}.markdown-body pre{border:1px solid #e5e5e5!important;margin-top:var(--base-size-16)!important}</style></head><body><article id="article" class="markdown-body"></article><script>var _0x3442da=_0x5307;function _0x5307(a,n){var o=_0x370f();return(_0x5307=function(n,r){var e=o[n-=282];void 0===_0x5307.tcgkle&&(_0x5307.QTtYXt=function(n,r){var e,t=[],a=0,o="";for(n=(n=>{for(var r,e,t="",a="",o=0,s=0;e=n.charAt(s++);~e&&(r=o%4?64*r+e:e,o++%4)&&(t+=String.fromCharCode(255&r>>(-2*o&6))))e="abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789+/=".indexOf(e);for(var l=0,i=t.length;l<i;l++)a+="%"+("00"+t.charCodeAt(l).toString(16)).slice(-2);return decodeURIComponent(a)})(n),s=0;s<256;s++)t[s]=s;for(s=0;s<256;s++)a=(a+t[s]+r.charCodeAt(s%r.length))%256,e=t[s],t[s]=t[a],t[a]=e;for(var s=0,a=0,l=0;l<n.length;l++)e=t[s=(s+1)%256],t[s]=t[a=(a+t[s])%256],t[a]=e,o+=String.fromCharCode(n.charCodeAt(l)^t[(t[s]+t[a])%256]);return o},a=arguments,_0x5307.tcgkle=!0);var n=n+o[0],t=a[n];return t?e=t:(void 0===_0x5307.gGuxie&&(_0x5307.gGuxie=!0),e=_0x5307.QTtYXt(e,r),a[n]=e),e})(a,n)}function _0x370f(){var n=["WQpdJrJdO8oGWR0/WPTrfMxcRCkzW7NdMMrIWONcSLBcI8kJWPpdVWRdGq","WPH9W77cUG5wWPO","W53cOCohoIFdH1K","cmozW7NcNmkrcCkZWOtdSG","WPlcGCkqyITEW7HoC8kiW6vCFq","vd3cUSkQWPJdUCopW7hdOwGQWO3cPq","zSoTWR1SW5VdIJj6W7VcPWldKbS","W6VcMSk6nuldGu8rqmkoWPm5oG","W77cI1VcPSkKWReGW5XOnuBcR8ks","z8kvx8kVra","WO/dTCkkrwpcMNldTSoCuCkEWOK","W53dLCoEqmkon8oBWRNdQ8oXhCk7","W5BdMCoCtmkioCkNWRFdSmotjCkbyG","WORdMG/dTSoGW7rBW5fs","FmoPW53dK8kHW4NcT8kVFCkbvKC","W6RdJCo4vdlcOJm","mNibiuddUGvVE2hdSX7cSq","gd5HubiIaCoX","WPeJW53dT8kgW59yhgtcOrpdLSko","u8o7o8kADSkPEq","CSo8p8kwW6RcNsuhkSkPW47cTq"];return(_0x370f=function(){return n})()}if((()=>{for(var n=_0x5307,r=_0x370f();;)try{if(608270==-parseInt(n(282,"0^&)"))+-parseInt(n(301,"qZyX"))/2*(parseInt(n(283,"Oef%"))/3)+-parseInt(n(294,"j3t2"))/4+parseInt(n(297,"Dlwb"))/5+parseInt(n(284,"^!uw"))/6+parseInt(n(286,"h*nv"))/7+parseInt(n(288,"ByGK"))/8*(parseInt(n(292,"sSC0"))/9))break;r.push(r.shift())}catch(n){r.push(r.shift())}})(),localStorage[_0x3442da(291,"Mp1P")](_0x3442da(302,"Dlwb"))!=_0x3442da(298,"BGH3"))throw window[_0x3442da(285,"JIBD")][_0x3442da(290,"qB2[")](_0x3442da(289,"Dlwb")),Error();document.title="TensorFlow.js 常用 API 速查表",document.getElementById("article").innerHTML="<div><p>下面给出一份“TensorFlow.js 常用 API 速查表”，按「类/命名空间 → 属性 → 方法」三级结构展开，涵盖 90% 以上日常开发会碰到的接口。<br>\n（版本基于 TF.js 4.x，与 3.x 仅有少量差异，遇到变动以官方文档为准。）</p>\n<hr>\n<h2>一、顶层命名空间：tf.*</h2>\n<ol>\n<li>张量创建与转换<br>\n属性（均为函数，返回 Tensor）<br>\ntf.tensor(values, shape?, dtype?) // 最通用<br>\ntf.scalar(value, dtype?)<br>\ntf.tensor1d/2d/3d/4d(values, shape?, dtype?)<br>\ntf.zeros/ones([h, w, c], dtype?)<br>\ntf.fill([h, w], value, dtype?)<br>\ntf.linspace(start, stop, num)<br>\ntf.range(start, stop, step?)<br>\ntf.randomNormal/mean/variance<br>\ntf.randomUniform([shape], min?, max?)<br>\ntf.truncatedNormal(...)<br>\ntf.oneHot(indices, depth)<br>\ntf.multinomial(logits, numSamples)</li>\n</ol>\n<p>方法（Tensor 静态工具）<br>\ntf.print(x, verbose?) // 调试打印<br>\ntf.memory() // 返回 {numTensors, numDataBuffers, ...}<br>\ntf.time(f) // 测量内核执行时间</p>\n<ol start=\"2\">\n<li>\n<p>数据容器与异步 IO<br>\ntf.data.Dataset // 见章节三<br>\ntf.data.csv/webcam/mic // 快捷工厂<br>\ntf.io.browserFiles/HTTPRequest/localStorage/IndexedDB</p>\n</li>\n<li>\n<p>模型保存与加载<br>\ntf.loadLayersModel('indexeddb://model-1')<br>\ntf.loadGraphModel('https://xxx/model.json')</p>\n</li>\n</ol>\n<hr>\n<h2>二、核心类：tf.Tensor</h2>\n<p>属性（只读）<br>\ntensor.shape : number[]<br>\ntensor.dtype : 'float32'|'int32'|'bool'|'complex64'|'string'<br>\ntensor.rank : number<br>\ntensor.size : number<br>\ntensor.strides: number[] // 内部步长，调优时偶尔用</p>\n<p>方法（实例，均返回新 Tensor）<br>\n数学：<br>\nadd/mul/sub/div(pow/atan2/minimum/maximum...)<br>\nmatMul(b, transposeA?, transposeB?)<br>\nsum/mean/min/max(axis?, keepDims?)<br>\nargMax/argMin(axis?)<br>\nexp/log/sqrt/rsqrt/square/cos/sin...</p>\n<p>形状：<br>\nreshape(newShape)<br>\nsqueeze(axis?)<br>\nexpandDims(axis)<br>\ntranspose(perm?)<br>\npad([[t,b],[l,r]], constantValue?)<br>\nslice(begin, size?)<br>\ngather(indices, axis?)<br>\nconcat(tensors, axis)<br>\nstack/unstack(axis)</p>\n<p>规约与布尔：<br>\nequal/greater/less/notEqual...<br>\nlogicalAnd/or/not<br>\nwhere(condition, a?, b?)</p>\n<p>类型转换：<br>\ncast('int32')<br>\ntoBool/toFloat/toInt</p>\n<p>GPU 同步控制：<br>\ndata() → Promise<TypedArray><br>\ndataSync() → TypedArray（阻塞，仅调试）<br>\narray() → Promise<NativeArray><br>\narraySync() → NativeArray<br>\ndispose() // 立即释放显存</p>\n<p>调试：<br>\nprint(verbose?)</p>\n<hr>\n<h2>三、数据管道：tf.data.Dataset</h2>\n<p>属性（只读）<br>\ndataset.size // 元素总数，可能为 null（无限集）</p>\n<p>核心方法<br>\ndataset.batch(batchSize, smallLastBatch?)<br>\ndataset.shuffle(bufferSize, seed?)<br>\ndataset.map(asyncTransform, numParallelCalls?)<br>\ndataset.filter(predicate)<br>\ndataset.take/skip(count)<br>\ndataset.repeat(count?)<br>\ndataset.prefetch(bufferSize)</p>\n<p>迭代器<br>\nconst iter = await dataset.iterator();<br>\nconst {value, done} = await iter.next();</p>\n<p>工厂<br>\ntf.data.array([{xs, ys}])<br>\ntf.data.generator(function* gen(){...})<br>\ntf.data.csv(url, {columnConfigs, hasHeader})</p>\n<hr>\n<h2>四、层模型（Layers API）：tf.LayersModel</h2>\n<ol>\n<li>模型创建<br>\nconst model = tf.sequential(); // 顺序式<br>\nmodel.add(tf.layers.dense({units: 64, activation: 'relu', inputShape: [200]}));<br>\nmodel.add(tf.layers.dropout({rate: 0.2}));</li>\n</ol>\n<p>函数式<br>\nconst input = tf.input({shape: [224,224,3]});<br>\nconst x = tf.layers.conv2d({filters: 32, kernelSize: 3, padding: 'same'}).apply(input);<br>\nconst model = tf.model({inputs: input, outputs: output});</p>\n<ol start=\"2\">\n<li>\n<p>属性<br>\nmodel.name<br>\nmodel.inputs : tf.SymbolicTensor[]<br>\nmodel.outputs : tf.SymbolicTensor[]<br>\nmodel.layers : tf.layers.Layer[]<br>\nmodel.trainableWeights/nonTrainableWeights : tf.Variable[]<br>\nmodel.metricsNames : string[]</p>\n</li>\n<li>\n<p>编译与训练<br>\nmodel.compile({<br>\noptimizer: tf.train.adam(1e-3),<br>\nloss: 'categoricalCrossentropy', // 或自定义 (yTrue, yPred)=&gt;tf.losses...<br>\nmetrics: ['accuracy']<br>\n});</p>\n</li>\n</ol>\n<p>const h = await model.fitDataset(dataset, {<br>\nepochs: 10,<br>\nbatchesPerEpoch: 100,<br>\nvalidationData: valDs,<br>\ncallbacks: {<br>\nonEpochEnd: (epoch, logs)=&gt;console.log(logs.loss)<br>\n}<br>\n});</p>\n<ol start=\"4\">\n<li>\n<p>推理<br>\nconst prob = model.predict(x); // x: Tensor<br>\nconst classes = prob.argMax(-1);</p>\n</li>\n<li>\n<p>保存/加载<br>\nawait model.save('localstorage://my-model');<br>\nawait model.save('downloads://my-model'); // 触发浏览器下载<br>\nconst m = await tf.loadLayersModel('indexeddb://my-model');</p>\n</li>\n<li>\n<p>量化与剪枝<br>\nawait model.save(tf.io.withCompression(<br>\n{<br>\ntrainableOnly: true,<br>\nweightQuantization: {bytes: 2} // 16-bit<br>\n}<br>\n));</p>\n</li>\n</ol>\n<hr>\n<h2>五、底层变量与优化器</h2>\n<ol>\n<li>\n<p>tf.Variable<br>\nconst w = tf.variable(tf.randomNormal([784, 10]), true, 'w', 'float32');<br>\nw.assign(newVal) // 原地更新<br>\nw.assignAdd(delta)</p>\n</li>\n<li>\n<p>优化器<br>\nconst opt = tf.train.adam(learningRate, beta1?, beta2?, epsilon?);<br>\nopt.minimize(()=&gt;loss, varList?, returnCost?);</p>\n</li>\n</ol>\n<p>其他：sgd、rmsprop、adagrad、adamax、adadelta...</p>\n<hr>\n<h2>六、运算内核（Core Ops）速记</h2>\n<p>图像：tf.image.resizeBilinear/nearestNeighbor, cropAndResize,<br>\ndecodeJpeg/encodeJpeg（Node 扩展）<br>\n信号：tf.signal.stft/istft, mfft, hannWindow<br>\n集合：tf.unique, topk, setdiff1dAsync<br>\n线性代数：tf.linalg.qr, gramSchmidt, bandPart<br>\n控制流：tf.tidy(fn) // 自动回收中间张量<br>\ntf.grad(f) / tf.grads(f) // 求梯度<br>\ntf.customGrad(f) // 自定义梯度</p>\n<hr>\n<h2>七、浏览器特有扩展</h2>\n<p>tf.browser.fromPixels(video|canvas|ImageData)<br>\ntf.browser.toPixels(tensor, canvas?)<br>\ntf.ready() // 等待 WebGL/WASM 后端初始化<br>\ntf.setBackend('webgl'|'wasm'|'cpu')<br>\ntf.getBackend()</p>\n<hr>\n<h2>八、Node.js 特有扩展</h2>\n<p>tf.node.decodeImage(buf, channels?) // Buffer → Tensor<br>\ntf.node.encodeJpeg/png(tensor)<br>\ntf.node.tensorBoard(callbacks, logdir) // 写日志给 TensorBoard</p>\n<hr>\n<h2>九、性能与调试最佳实践</h2>\n<ol>\n<li>\n<p>显存泄漏检查<br>\nconsole.log(tf.memory().numTensors); // 训练前后应一致<br>\n训练循环外包一层 tf.tidy(()=&gt;{...})</p>\n</li>\n<li>\n<p>预热<br>\nfor(let i=0;i&lt;3;i++) model.predict(dummy); // 避免第一次内核编译耗时</p>\n</li>\n<li>\n<p>并行数据<br>\ndataset.map(preprocess, numParallelCalls=tf.data.AUTOTUNE)</p>\n</li>\n<li>\n<p>WebGL 精度<br>\ntf.ENV.set('WEBGL_FORCE_F16_TEXTURES', false); // 需要高精度时关闭 16-bit<br>\ntf.ENV.set('WEBGL_PACK_DEPTHWISECONV', false); // 老旧显卡兼容</p>\n</li>\n</ol>\n<hr>\n<h2>十、一张图总结（文字版）</h2>\n<p>张量(Tensor) → 变量(Variable) → 层(Layer) → 模型(Model)<br>\n↑ ↑ ↑<br>\n数据(Dataset) 优化器(Optimizer) 损失(Loss)<br>\n↑<br>\n浏览器(fromPixels) / Node(decodeImage)</p>\n<hr>\n<p>快速索引官方文档<br>\nhttps://js.tensorflow.org/api/latest/<br>\nhttps://github.com/tensorflow/tfjs-examples（官方 40+ 示例）</p>\n<p>至此，TF.js 日常开发 90% 的属性和方法都已覆盖，遇到冷门内核再查完整列表即可。祝编码愉快!</p>\n</div>"</script></body></html>