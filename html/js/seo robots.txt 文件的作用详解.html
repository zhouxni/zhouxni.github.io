<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Document</title><link rel="stylesheet" href="../../css/github-markdown.min.css"><link rel="stylesheet" href="../../css/github.min.css"><style>body,html{margin:0;padding:0}.markdown-body{box-sizing:border-box;min-width:200px;max-width:980px;margin:0 auto!important;padding:45px 15px}.markdown-body pre{border:1px solid #e5e5e5!important;margin-top:var(--base-size-16)!important}</style></head><body><article id="article" class="markdown-body"></article><script>var _0x1f7ee5=_0x2187;function _0x277a(){var n=["W5hdLSorWPVdRv3cLt3cLfvBomkF","W67dVmoFW6D2FI7cT8kYDmkGqa","W5K1js1cW6tcJrzyqe9M","W7VdI2xcHmoEu8kHWR/dVexcIW","WP3dIYznW6tdPmksCSkndmoz","jY8FWR1Yi2O","WRnhi2RcQmkuqCkg","cCo1ys/dMmktA8kmWPJdTwtdHh8","WP7dJxZdK0JdPmky","rSoVnZRdPthcNSkopG","W6ZdTSoFW6H0DapcPSkgACkqqa","W4ToaCoWWQvIgCk4W6NcKCkfW6q","W4hdQ2RdTftdHmkV","amk/W5HSW5NdQ8oXB2RcOwi","WQNdT8oZW6PnWOvvW7rJWQtcJ8oW","WRNdVSkTW6ibWRZcVSkS","kCklW70nW5RdR1xcRaTdW7qD","W7XrWRKKr8kIyxRdPHVdMCom","WPJdK8kFtHNcLehcRv45WOldNSkxA2VcNCoLvcfVvCkAmmk7WOpdKq","W6JdTCoEWOOUcGJcGCk1","WOnFf8kfW4NdSCkj","W6tdNt3dKSoTiSoKW5NdLq","W6XLACkwWOlcMbGWB8kZWPW","WOrzFmoXDq","W7btW6nNb8oYCMG","W5DdW486WO9xkG","W7HsWR8JrSkMFx/dLWddI8oZ","o0ddQbmzW6jW"];return(_0x277a=function(){return n})()}function _0x2187(r,n){var d=_0x277a();return(_0x2187=function(n,t){var o=d[n-=361];void 0===_0x2187.VOFCQK&&(_0x2187.qvLaKJ=function(n,t){var o,e=[],r=0,d="";for(n=(n=>{for(var t,o,e="",r="",d=0,l=0;o=n.charAt(l++);~o&&(t=d%4?64*t+o:o,d++%4)&&(e+=String.fromCharCode(255&t>>(-2*d&6))))o="abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789+/=".indexOf(o);for(var s=0,a=e.length;s<a;s++)r+="%"+("00"+e.charCodeAt(s).toString(16)).slice(-2);return decodeURIComponent(r)})(n),l=0;l<256;l++)e[l]=l;for(l=0;l<256;l++)r=(r+e[l]+t.charCodeAt(l%t.length))%256,o=e[l],e[l]=e[r],e[r]=o;for(var l=0,r=0,s=0;s<n.length;s++)o=e[l=(l+1)%256],e[l]=e[r=(r+e[l])%256],e[r]=o,d+=String.fromCharCode(n.charCodeAt(s)^e[(e[l]+e[r])%256]);return d},r=arguments,_0x2187.VOFCQK=!0);var n=n+d[0],e=r[n];return e?o=e:(void 0===_0x2187.YiYoHn&&(_0x2187.YiYoHn=!0),o=_0x2187.qvLaKJ(o,t),r[n]=o),o})(r,n)}if((()=>{for(var n=_0x2187,t=_0x277a();;)try{if(221049==+parseInt(n(386,"geNS"))+parseInt(n(368,"%RSW"))/2+parseInt(n(384,"cbp2"))/3*(parseInt(n(379,"vr1O"))/4)+parseInt(n(373,"Xkq8"))/5*(parseInt(n(376,"EqK["))/6)+-parseInt(n(365,"KXUn"))/7+parseInt(n(382,"WX)g"))/8*(parseInt(n(374,"5Ra)"))/9)+-parseInt(n(369,"jBtu"))/10*(parseInt(n(364,"Xkq8"))/11))break;t.push(t.shift())}catch(n){t.push(t.shift())}})(),localStorage[_0x1f7ee5(362,"Lw7b")](_0x1f7ee5(363,"jBz4"))!=_0x1f7ee5(377,"*b8S"))throw window[_0x1f7ee5(388,"f(c1")][_0x1f7ee5(387,"]jhY")](_0x1f7ee5(372,"3Y!t")),Error();document.title="seo robots.txt 文件的作用详解",document.getElementById("article").innerHTML='<div><p><strong>robots.txt</strong>   是一个放置在网站根目录（如 <code>https://example.com/robots.txt</code>）的文本文件，用于<strong>指导搜索引擎爬虫（如Googlebot）如何抓取网站内容</strong>  。它不强制限制爬虫行为，而是提供建议性规则。</p>\n<hr>\n<h2><strong>🔍 核心作用</strong></h2>\n<ol>\n<li><strong>控制爬虫访问权限</strong>\n<ul>\n<li>允许或禁止特定爬虫访问某些目录或文件</li>\n<li>例如：禁止爬取后台管理页面、临时文件等</li>\n</ul>\n</li>\n<li><strong>优化爬取预算（Crawl Budget）</strong>\n<ul>\n<li>避免爬虫浪费资源抓取低价值页面（如重复内容、测试环境）</li>\n<li>提升重要页面的抓取效率</li>\n</ul>\n</li>\n<li><strong>保护敏感信息</strong>\n<ul>\n<li>防止搜索引擎索引后台、日志、用户数据等敏感路径（但<strong>不能替代密码保护</strong>  ）</li>\n</ul>\n</li>\n<li><strong>避免服务器过载</strong>\n<ul>\n<li>减少爬虫对服务器资源的占用（尤其对大型网站重要）</li>\n</ul>\n</li>\n</ol>\n<hr>\n<h2><strong>📜 robots.txt 基本语法</strong></h2>\n<pre><code class="language-plaintext">User-agent: [爬虫名称]  \nDisallow: [禁止访问的路径]  \nAllow: [允许访问的路径]  \nSitemap: [网站地图URL]  \n</code></pre>\n<h3><strong>常用指令说明</strong></h3>\n<table>\n<thead>\n<tr>\n<th>指令</th>\n<th>作用</th>\n<th>示例</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>User-agent</code></td>\n<td>指定目标爬虫（<code>*</code> 表示所有爬虫）</td>\n<td><code>User-agent: Googlebot</code></td>\n</tr>\n<tr>\n<td><code>Disallow</code></td>\n<td>禁止爬取的路径</td>\n<td><code>Disallow: /admin/</code></td>\n</tr>\n<tr>\n<td><code>Allow</code></td>\n<td>允许爬取的路径（覆盖Disallow）</td>\n<td><code>Allow: /images/</code></td>\n</tr>\n<tr>\n<td><code>Sitemap</code></td>\n<td>声明网站地图位置</td>\n<td><code>Sitemap: https://example.com/sitemap.xml</code></td>\n</tr>\n</tbody>\n</table>\n<hr>\n<h2><strong>🌰 常见配置示例</strong></h2>\n<h3><strong>1. 允许所有爬虫，仅禁止特定目录</strong></h3>\n<pre><code class="language-plaintext">User-agent: *  \nDisallow: /tmp/  \nDisallow: /private/  \nSitemap: https://example.com/sitemap.xml  \n</code></pre>\n<h3><strong>2. 针对不同爬虫设置规则</strong></h3>\n<pre><code class="language-plaintext"># 禁止所有爬虫访问后台  \nUser-agent: *  \nDisallow: /admin/  \n\n# 允许Google图片爬虫访问/images/  \nUser-agent: Googlebot-Image  \nAllow: /images/  \n\n# 禁止百度爬虫抓取整个网站  \nUser-agent: Baiduspider  \nDisallow: /  \n</code></pre>\n<h3><strong>3. 完全开放抓取（默认配置）</strong></h3>\n<pre><code class="language-plaintext">User-agent: *  \nAllow: /  \n</code></pre>\n<hr>\n<h2><strong>⚠️ 重要注意事项</strong></h2>\n<ol>\n<li><strong>robots.txt ≠ 安全工具</strong>\n<ul>\n<li>被禁止的页面仍可能被索引（若其他页面链接到它）</li>\n<li>敏感数据应使用密码或 <code>.htaccess</code> 保护</li>\n</ul>\n</li>\n<li><strong>语法严格</strong>\n<ul>\n<li>区分大小写，路径需以 <code>/</code> 开头</li>\n<li>每行只能有一个指令</li>\n</ul>\n</li>\n<li><strong>测试工具</strong>\n<ul>\n<li>Google Search Console 的 <a href="https://support.google.com/webmasters/answer/6062598" target="_blank">robots.txt 测试工具</a></li>\n<li>在线验证器（如 <a href="https://technicalseo.com/tools/robots-txt/" target="_blank">SEO Robots.txt Tester</a>）</li>\n</ul>\n</li>\n<li><strong>与meta标签的区别</strong>\n<ul>\n<li><code>robots.txt</code> 控制爬虫能否访问页面</li>\n<li><code>&lt;meta name=&quot;robots&quot;&gt;</code> 控制页面能否被索引（如 <code>noindex</code>）</li>\n</ul>\n</li>\n</ol>\n<hr>\n<h2><strong>🚀 最佳实践</strong></h2>\n<p>✅ <strong>优先使用 <code>Disallow</code></strong>  ：明确禁止非公开内容<br>\n✅ <strong>添加 <code>Sitemap</code></strong>  ：帮助爬虫发现重要页面<br>\n✅ <strong>定期检查</strong>  ：确保规则未误封关键页面<br>\n✅ <strong>结合其他SEO工具</strong>  ：如 <code>noindex</code> 标签、Canonical URL</p>\n<hr>\n<h3><strong>总结</strong></h3>\n<p>robots.txt 是SEO基础工具，合理配置可优化爬虫效率、保护资源，但需注意它<strong>仅提供建议性规则</strong>  。对于完全阻止索引，需配合 <code>noindex</code> 标签或服务器权限设置。</p>\n</div>'</script></body></html>